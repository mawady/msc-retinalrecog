{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "RxNcDSRXknR8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt \n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "bWHtA3idktPX"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/My Drive/University Of Stirling/Dissertation/retinal-rec/Datasets/APTOS2019'\n",
        "TRAIN_PATH = DATASET_PATH + \"/train_images/\"\n",
        "TRAIN_PREP_PATH = DATASET_PATH + \"/train_preprocessed/\"\n",
        "classes = [ \"No DR\", \"Mild\", \"Moderate\", \"Severe\", \"Proliferative DR\" ]\n",
        "N_CLASSES = 5\n",
        "IMG_SIZE = (512, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFw5FXuIk32k",
        "outputId": "df12cb6e-912b-4e88-f124-1cf6df74cf2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "CWD: /content/drive/My Drive/University Of Stirling/Dissertation/retinal-rec/Datasets/APTOS2019\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(DATASET_PATH)\n",
        "print(\"CWD:\",os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGwGGMcyxiti"
      },
      "source": [
        "In preparation for the NN, it is necessary to create the correct directory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "IUzIzdkex1gm"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DATASET_PATH + \"/train\"):\n",
        "  os.mkdir(DATASET_PATH + \"/train\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/0\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/1\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/2\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/3\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/4\")\n",
        "\n",
        "if not os.path.exists(DATASET_PATH + \"/validation\"):\n",
        "  os.mkdir(DATASET_PATH + \"/validation\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/0\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/1\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/2\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/3\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/4\")\n",
        "\n",
        "if not os.path.exists(DATASET_PATH + \"/test\"):\n",
        "  os.mkdir(DATASET_PATH + \"/test\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/0\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/1\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/2\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/3\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4KAjfl4Tk9UM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f49448e-8f69-419c-ac5a-fb0282cecb88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2050, 2) (879, 2) (733, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "def read_dataset():\n",
        "  df = pd.read_csv(DATASET_PATH + \"/train.csv\")\n",
        "  \n",
        "  # Train-test split\n",
        "  train, test = train_test_split(df, test_size=.2)\n",
        "  train, valid = train_test_split(train, test_size=.3)\n",
        "\n",
        "  for image in train.values:\n",
        "    shutil.copyfile(TRAIN_PREP_PATH + image[0] + \".png\", DATASET_PATH + \"/train/\" + str(image[1]) + \"/\" + image[0] + \".png\")\n",
        "\n",
        "  for image in valid.values:\n",
        "    shutil.copyfile(TRAIN_PREP_PATH + image[0] + \".png\", DATASET_PATH + \"/validation/\" + str(image[1]) + \"/\" + image[0] + \".png\")\n",
        "\n",
        "  for image in test.values:\n",
        "    shutil.copyfile(TRAIN_PREP_PATH + image[0] + \".png\", DATASET_PATH + \"/test/\" + str(image[1]) + \"/\" + image[0] + \".png\")\n",
        "  \n",
        "  return (train, valid, test)\n",
        "\n",
        "\n",
        "train, valid, test = read_dataset()\n",
        "\n",
        "print(train.shape, valid.shape, test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdri7jX-27Pu",
        "outputId": "e2c93afc-f6d1-4556-d2ad-b0efe24ce0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2050 images belonging to 5 classes.\n",
            "Found 879 images belonging to 5 classes.\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_61 (Conv2D)          (None, 62, 62, 128)       46592     \n",
            "                                                                 \n",
            " batch_normalization_60 (Bat  (None, 62, 62, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_37 (MaxPoolin  (None, 31, 31, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_62 (Conv2D)          (None, 31, 31, 256)       819456    \n",
            "                                                                 \n",
            " batch_normalization_61 (Bat  (None, 31, 31, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_38 (MaxPoolin  (None, 10, 10, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_63 (Conv2D)          (None, 10, 10, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_62 (Bat  (None, 10, 10, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_64 (Conv2D)          (None, 10, 10, 256)       65792     \n",
            "                                                                 \n",
            " batch_normalization_63 (Bat  (None, 10, 10, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_65 (Conv2D)          (None, 10, 10, 256)       65792     \n",
            "                                                                 \n",
            " batch_normalization_64 (Bat  (None, 10, 10, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_39 (MaxPoolin  (None, 5, 5, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 1024)              6554624   \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 5)                 5125      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,201,669\n",
            "Trainable params: 9,199,365\n",
            "Non-trainable params: 2,304\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "65/65 [==============================] - 357s 5s/step - loss: 3.3998 - accuracy: 0.4727 - val_loss: 26.5473 - val_accuracy: 0.4653\n",
            "Epoch 2/10\n",
            "65/65 [==============================] - 358s 6s/step - loss: 1.3813 - accuracy: 0.5785 - val_loss: 3.3573 - val_accuracy: 0.4653\n",
            "Epoch 3/10\n",
            "65/65 [==============================] - 354s 5s/step - loss: 1.1733 - accuracy: 0.6015 - val_loss: 1.2465 - val_accuracy: 0.4790\n",
            "Epoch 4/10\n",
            "65/65 [==============================] - 350s 5s/step - loss: 1.0172 - accuracy: 0.6493 - val_loss: 1.2173 - val_accuracy: 0.5631\n",
            "Epoch 5/10\n",
            "65/65 [==============================] - 351s 5s/step - loss: 0.9504 - accuracy: 0.6707 - val_loss: 1.7455 - val_accuracy: 0.4949\n",
            "Epoch 6/10\n",
            "65/65 [==============================] - 349s 5s/step - loss: 1.0582 - accuracy: 0.6244 - val_loss: 1.5116 - val_accuracy: 0.4972\n",
            "Epoch 7/10\n",
            "65/65 [==============================] - 351s 5s/step - loss: 0.9991 - accuracy: 0.6537 - val_loss: 1.2005 - val_accuracy: 0.6007\n",
            "Epoch 8/10\n",
            "65/65 [==============================] - 353s 5s/step - loss: 0.9000 - accuracy: 0.6883 - val_loss: 1.5958 - val_accuracy: 0.5438\n",
            "Epoch 9/10\n",
            "65/65 [==============================] - 348s 5s/step - loss: 0.8446 - accuracy: 0.7000 - val_loss: 1.2816 - val_accuracy: 0.5154\n",
            "Epoch 10/10\n",
            "65/65 [==============================] - 349s 5s/step - loss: 0.8559 - accuracy: 0.6980 - val_loss: 2.4784 - val_accuracy: 0.4858\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8e4f65fb90>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH + \"/train.csv\")\n",
        "  \n",
        "# Train-test split\n",
        "#train, test = train_test_split(df, test_size=.2)\n",
        "\n",
        "# create a data generator\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "# load and iterate training dataset\n",
        "train_it = datagen.flow_from_directory(DATASET_PATH + \"/train/\", class_mode='categorical', batch_size=32)\n",
        "valid_it = datagen.flow_from_directory(DATASET_PATH + \"/validation/\", class_mode='categorical', batch_size=32)\n",
        "\n",
        "model=keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(256,256,3)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3)),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1024,activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1024,activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(5,activation='softmax')  \n",
        "])\n",
        "\n",
        "#model.build((256,512,512,3))\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001), metrics=[\"accuracy\"], run_eagerly=True)\n",
        "\n",
        "\n",
        "model.fit(train_it, validation_data=valid_it, epochs=10, verbose=1) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"class 2\")\n",
        "num_samples = 20\n",
        "for file in os.listdir(DATASET_PATH + \"/test/2\"):\n",
        "    img = cv2.imread(DATASET_PATH + \"/test/2/\" + file)\n",
        "    img = cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    prediction = model.predict(x=[img])\n",
        "    print(np.argmax(prediction))\n",
        "    num_samples = num_samples-1\n",
        "    if(num_samples == 0): \n",
        "      break"
      ],
      "metadata": {
        "id": "yOoyB9Q5Kvet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6096d181-ffc6-4cb4-8e2c-e94df6527b88"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class 2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C222Z5Gzn5w2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhCXYmJz/OLOR/vC6LAWEq"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}