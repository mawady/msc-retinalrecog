{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_prep2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QNwZ7LwPlyVt"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyObTGSbsLaR8Ot8BPUjhAAx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5efb78ce29164befb012959ab52efcf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c9a44fa6e7a4917a1aacc5cab77a087",
              "IPY_MODEL_af7f7af0e50c4e7cb499b654ae0076d1",
              "IPY_MODEL_4e712d4164a64671b4d16c05b36270cf"
            ],
            "layout": "IPY_MODEL_8db8e321439043d0897e88e4e0ce9d1b"
          }
        },
        "3c9a44fa6e7a4917a1aacc5cab77a087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0321c672d588498fbb3e1dbbc565dfa2",
            "placeholder": "​",
            "style": "IPY_MODEL_be3706ba2d5e4389b596dbbedbe2d81e",
            "value": "100%"
          }
        },
        "af7f7af0e50c4e7cb499b654ae0076d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67684cb8a529489bbb91d44b7c8f68f1",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd7246dba03d46e7ae5ea386c28ba24e",
            "value": 46830571
          }
        },
        "4e712d4164a64671b4d16c05b36270cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fd858b917c04228ab87199081a0c302",
            "placeholder": "​",
            "style": "IPY_MODEL_39aac1798323441ca9ae0f938b5be824",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 69.0MB/s]"
          }
        },
        "8db8e321439043d0897e88e4e0ce9d1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0321c672d588498fbb3e1dbbc565dfa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3706ba2d5e4389b596dbbedbe2d81e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67684cb8a529489bbb91d44b7c8f68f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd7246dba03d46e7ae5ea386c28ba24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fd858b917c04228ab87199081a0c302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39aac1798323441ca9ae0f938b5be824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ConvNEXT for DR detection"
      ],
      "metadata": {
        "id": "BqTdYbiDufny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "v6T0uIkA2aCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive Access"
      ],
      "metadata": {
        "id": "FqjI4jX32eZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8wasxZn3ue1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0286e1ec-1715-4ced-ed57-8f295a77fe5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "CWD: /content/drive/My Drive/University Of Stirling/Dissertation/ConvNEXT/APTOS2019\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Parameters\n",
        "DATASET_PATH = '/content/drive/My Drive/University Of Stirling/Dissertation/ConvNEXT/APTOS2019'\n",
        "PREP_PATH = DATASET_PATH + '/preprocessed_2/'\n",
        "MODEL_PATH= '/content/drive/My Drive/University Of Stirling/Dissertation/ConvNEXT/checkpoints/checkpoint_ResNet.pth'\n",
        "\n",
        "# Load Dataset From Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(DATASET_PATH)\n",
        "print(\"CWD:\",os.getcwd())\n",
        "\n",
        "if not os.path.exists(PREP_PATH):\n",
        "  os.mkdir(PREP_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Download"
      ],
      "metadata": {
        "id": "QNwZ7LwPlyVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Dataset with already preprocessed images\n",
        "!gdown --id 1TqlyF-VVLcuebqQcWuAKnG7ay3IKjylC\n",
        "!unzip APTOS2019.zip\n",
        "\n",
        "# Parameters\n",
        "DATASET_PATH = '/content/APTOS2019'\n",
        "PREP_PATH = DATASET_PATH + \"/preprocessed/\"\n",
        "MODEL_PATH= '/content/checkpoints/checkpoint.pth'\n",
        "\n",
        "if not os.path.exists(PREP_PATH):\n",
        "  os.mkdir(PREP_PATH)\n",
        "\n",
        "if not os.path.exists(\"/checkpoints\"):\n",
        "  os.mkdir(\"/checkpoints\")\n",
        "'''"
      ],
      "metadata": {
        "id": "rkDiqk__l2WY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6629ea24-93d7-43aa-8644-61f02a2b9835"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Dataset with already preprocessed images\\n!gdown --id 1TqlyF-VVLcuebqQcWuAKnG7ay3IKjylC\\n!unzip APTOS2019.zip\\n\\n# Parameters\\nDATASET_PATH = \\'/content/APTOS2019\\'\\nPREP_PATH = DATASET_PATH + \"/preprocessed/\"\\nMODEL_PATH= \\'/content/checkpoints/checkpoint.pth\\'\\n\\nif not os.path.exists(PREP_PATH):\\n  os.mkdir(PREP_PATH)\\n\\nif not os.path.exists(\"/checkpoints\"):\\n  os.mkdir(\"/checkpoints\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup"
      ],
      "metadata": {
        "id": "7En8mOAo3LJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm_dWwHWWU7U",
        "outputId": "56ab4ec1-7861-4788-a814-dab74fd42b23"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and global parameters"
      ],
      "metadata": {
        "id": "ifpMFFxX2lkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader \n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.transforms as transforms\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "M8h_iHOx2sX5",
        "outputId": "5a008ef2-b280-49de-f301-857face10339"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 512\n",
        "BATCH_SIZE = 10\n",
        "NUM_EPOCHS = 30\n",
        "NUM_INIT_EPOCHS = 4\n",
        "LR_INIT = 0.0001\n",
        "LR = 0.0001\n",
        "DECAY_FACTOR = 0.1\n",
        "PATIENCE = 3\n",
        "ES_PATIENCE = 3\n",
        "LR_TSH = 0.000001\n",
        "TSH_MODE = 'abs'\n",
        "N_CLASSES = 5\n",
        "CLASSES = [ \"No DR\", \"Mild\", \"Moderate\", \"Severe\", \"Proliferative DR\" ]"
      ],
      "metadata": {
        "id": "c4XnQDs22u1c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "fIl8zMTjz05Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import multiprocessing\n",
        "from multiprocessing.pool import ThreadPool\n",
        "\n",
        "# ref: https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy/notebook\n",
        "# ref: https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy/notebook\n",
        "# ref for circle crop: https://github.com/debayanmitra1993-data/Blindness-Detection-Diabetic-Retinopathy-/blob/master/research_paper_implementation.ipynb\n",
        "def crop_image_from_gray(img,tol=7):\n",
        "    if img.ndim ==2:\n",
        "        mask = img>tol\n",
        "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "    elif img.ndim==3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mask = gray_img>tol\n",
        "        \n",
        "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
        "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
        "            return img # return original image\n",
        "        else:\n",
        "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
        "    #         print(img1.shape,img2.shape,img3.shape)\n",
        "            img = np.stack([img1,img2,img3],axis=-1)\n",
        "    #         print(img.shape)\n",
        "        return img\n",
        "\n",
        "def circle_crop(img, sigmaX = 30):   \n",
        "    \"\"\"\n",
        "    Create circular crop around image centre    \n",
        "    \"\"\"    \n",
        "    img = crop_image_from_gray(img)    \n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    height, width, depth = img.shape    \n",
        "    \n",
        "    x = int(width/2)\n",
        "    y = int(height/2)\n",
        "    r = np.amin((x,y))\n",
        "    \n",
        "    circle_img = np.zeros((height, width), np.uint8)\n",
        "    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n",
        "    img = cv2.bitwise_and(img, img, mask=circle_img)\n",
        "    img = crop_image_from_gray(img)\n",
        "    img=cv2.addWeighted(img,4, cv2.GaussianBlur( img , (0,0) , sigmaX) ,-4 ,128)\n",
        "    return img \n",
        "\n",
        "def preprocess(id_code):\n",
        "  path = DATASET_PATH + \"/train_images/\" + id_code + \".png\"\n",
        "\n",
        "  if(os.path.isfile(path) == False):\n",
        "    print(id_code + \" does not exist!\")\n",
        "    return\n",
        "\n",
        "  img = cv2.imread(path)\n",
        "\n",
        "  # Circle crop\n",
        "  img = circle_crop(img)\n",
        "\n",
        "  cv2.imwrite(PREP_PATH + id_code + \".png\", cv2.resize(img, (IMG_SIZE,IMG_SIZE)))\n",
        "\n",
        "\n",
        "def multiprocessor(process:int, imgs:list):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        process: (int) number of process to run\n",
        "        imgs:(list) list of images\n",
        "    \"\"\"\n",
        "    print(f'MESSAGE: Running {process} process')\n",
        "    results = ThreadPool(process).map(preprocess, imgs)\n",
        "    return results\n",
        "\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "\n",
        "multiprocessor(4, list(dataset.id_code.values))\n",
        "'''"
      ],
      "metadata": {
        "id": "IDgCkcf1z0NF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "ff4f2ac3-b69a-4d46-f37a-d656e92cba58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport multiprocessing\\nfrom multiprocessing.pool import ThreadPool\\n\\n# ref: https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy/notebook\\n# ref: https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy/notebook\\n# ref for circle crop: https://github.com/debayanmitra1993-data/Blindness-Detection-Diabetic-Retinopathy-/blob/master/research_paper_implementation.ipynb\\ndef crop_image_from_gray(img,tol=7):\\n    if img.ndim ==2:\\n        mask = img>tol\\n        return img[np.ix_(mask.any(1),mask.any(0))]\\n    elif img.ndim==3:\\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\\n        mask = gray_img>tol\\n        \\n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\\n        if (check_shape == 0): # image is too dark so that we crop out everything,\\n            return img # return original image\\n        else:\\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\\n    #         print(img1.shape,img2.shape,img3.shape)\\n            img = np.stack([img1,img2,img3],axis=-1)\\n    #         print(img.shape)\\n        return img\\n\\ndef circle_crop(img, sigmaX = 30):   \\n    \"\"\"\\n    Create circular crop around image centre    \\n    \"\"\"    \\n    img = crop_image_from_gray(img)    \\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n    \\n    height, width, depth = img.shape    \\n    \\n    x = int(width/2)\\n    y = int(height/2)\\n    r = np.amin((x,y))\\n    \\n    circle_img = np.zeros((height, width), np.uint8)\\n    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\\n    img = cv2.bitwise_and(img, img, mask=circle_img)\\n    img = crop_image_from_gray(img)\\n    img=cv2.addWeighted(img,4, cv2.GaussianBlur( img , (0,0) , sigmaX) ,-4 ,128)\\n    return img \\n\\ndef preprocess(id_code):\\n  path = DATASET_PATH + \"/train_images/\" + id_code + \".png\"\\n\\n  if(os.path.isfile(path) == False):\\n    print(id_code + \" does not exist!\")\\n    return\\n\\n  img = cv2.imread(path)\\n\\n  # Circle crop\\n  img = circle_crop(img)\\n\\n  cv2.imwrite(PREP_PATH + id_code + \".png\", cv2.resize(img, (IMG_SIZE,IMG_SIZE)))\\n\\n\\ndef multiprocessor(process:int, imgs:list):\\n    \"\"\"\\n    Inputs:\\n        process: (int) number of process to run\\n        imgs:(list) list of images\\n    \"\"\"\\n    print(f\\'MESSAGE: Running {process} process\\')\\n    results = ThreadPool(process).map(preprocess, imgs)\\n    return results\\n\\ndataset = pd.read_csv(\"train.csv\")\\n\\nmultiprocessor(4, list(dataset.id_code.values))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "class DrDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = os.path.join(self.root_dir, self.labels.iloc[idx, 0])\n",
        "        img = read_image(path + \".png\")\n",
        "        lbl = self.labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, lbl\n",
        "\n",
        "global_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Lambda(lambda image: image.convert('RGB')),\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5413734, 0.5413734, 0.5413734], [0.17313044, 0.17313044, 0.17313044])\n",
        "    ])\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Lambda(lambda image: image.convert('RGB')),\n",
        "    # Data Augmentation\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5413734, 0.5413734, 0.5413734], [0.17313044, 0.17313044, 0.17313044])\n",
        "    ])\n",
        "\n",
        "dataset = DrDetectionDataset(DATASET_PATH + \"/train.csv\", PREP_PATH, transform = global_transforms)"
      ],
      "metadata": {
        "id": "5-jyUfWeQsM1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train-test-validation split\n",
        "\n",
        "# Set manual seed for reproducible results\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# 60% train, 20% test, 20% valid\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_val_size = (len(dataset) - train_size)\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_val_size])\n",
        "\n",
        "train_size = int(len(train_dataset) - test_val_size)\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_size, test_val_size])\n",
        "\n",
        "train_dataset.dataset.dataset.transform = train_transforms\n",
        "\n",
        "# Use a Weighted Sampler to fix class imbalances in the training set\n",
        "class_count = np.zeros(5)\n",
        "for image, label in train_dataset:\n",
        "  class_count[label] +=1\n",
        "\n",
        "class_weights = 1. / class_count\n",
        "weights = np.array([])\n",
        "\n",
        "for image, label in train_dataset:\n",
        "  weights = np.append(weights, class_weights[label])\n",
        "  \n",
        "weights = torch.from_numpy(weights)\n",
        "\n",
        "weighted_sampler = WeightedRandomSampler(weights=weights,num_samples=len(train_dataset),replacement=True)\n",
        "\n",
        "print(\"Train size: \", len(train_dataset))\n",
        "print(\"Val size: \", len(valid_dataset))\n",
        "print(\"Test size: \", len(test_dataset))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=weighted_sampler)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ],
      "metadata": {
        "id": "D1yRiT6ycRw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78cfa7ee-e5fa-45d2-eea1-577da3f2d886"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size:  2196\n",
            "Val size:  733\n",
            "Test size:  733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "6y1Ocn4xUOWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Model"
      ],
      "metadata": {
        "id": "uFKc_uo9ontb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(512, N_CLASSES)\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "WV1jVxLtQl5o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5efb78ce29164befb012959ab52efcf6",
            "3c9a44fa6e7a4917a1aacc5cab77a087",
            "af7f7af0e50c4e7cb499b654ae0076d1",
            "4e712d4164a64671b4d16c05b36270cf",
            "8db8e321439043d0897e88e4e0ce9d1b",
            "0321c672d588498fbb3e1dbbc565dfa2",
            "be3706ba2d5e4389b596dbbedbe2d81e",
            "67684cb8a529489bbb91d44b7c8f68f1",
            "cd7246dba03d46e7ae5ea386c28ba24e",
            "2fd858b917c04228ab87199081a0c302",
            "39aac1798323441ca9ae0f938b5be824"
          ]
        },
        "outputId": "432f0d37-63fb-4a6f-db72-614fe02caa27"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5efb78ce29164befb012959ab52efcf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "adpyl8PApNTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Warmup Training"
      ],
      "metadata": {
        "id": "pzXFwn_6G0wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "#https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR_INIT)\n",
        "\n",
        "top_accuracy = 0.0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(NUM_INIT_EPOCHS):\n",
        "  train_loss = 0.0\n",
        "  \n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "\n",
        "  for data, labels in train_loader:\n",
        "    if torch.cuda.is_available():\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(data)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    train_total += labels.size(0)\n",
        "    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # print statistics\n",
        "    train_loss += loss.item()\n",
        "    \n",
        "  valid_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  for data, labels in valid_loader:\n",
        "    if torch.cuda.is_available():\n",
        "      data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "      outputs = model(data)\n",
        "      loss = loss_fn(outputs,labels)\n",
        "      valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  val_accuracy = (100 * val_correct) // val_total\n",
        "\n",
        "  print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Training Accuracy: {100 * train_correct // train_total}' )\n",
        "  print(f'\\t\\t Validation Loss: {valid_loss / len(valid_loader)} \\t\\t Validation Accuracy: {val_accuracy}')\n",
        "  if val_accuracy > top_accuracy:\n",
        "      print(f'Validation Accuracy Increased({top_accuracy:.6f}--->{val_accuracy:.6f}) \\t Saving The Model')\n",
        "      top_accuracy = val_accuracy\n",
        "      # Save Model\n",
        "      torch.save(model, MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "sETdkp7jHn1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea109550-88a5-4868-bd7c-6445a025a42b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t\t Training Loss: 0.9615981580181555 \t\t Training Accuracy: 60\n",
            "\t\t Validation Loss: 0.03781032481709042 \t\t Validation Accuracy: 64\n",
            "Validation Accuracy Increased(0.000000--->64.000000) \t Saving The Model\n",
            "Epoch 2 \t\t Training Loss: 0.7065909989178181 \t\t Training Accuracy: 72\n",
            "\t\t Validation Loss: 0.05467936316051999 \t\t Validation Accuracy: 65\n",
            "Validation Accuracy Increased(64.000000--->65.000000) \t Saving The Model\n",
            "Epoch 3 \t\t Training Loss: 0.5985241098498756 \t\t Training Accuracy: 78\n",
            "\t\t Validation Loss: 0.043568097256325385 \t\t Validation Accuracy: 67\n",
            "Validation Accuracy Increased(65.000000--->67.000000) \t Saving The Model\n",
            "Epoch 4 \t\t Training Loss: 0.4903069615025412 \t\t Training Accuracy: 82\n",
            "\t\t Validation Loss: 0.03119808032705977 \t\t Validation Accuracy: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "I-6pjfvpIH8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "#https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
        "#https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
        "#https://clay-atlas.com/us/blog/2021/08/25/pytorch-en-early-stopping/\n",
        "\n",
        "model = torch.load(MODEL_PATH).to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=DECAY_FACTOR, patience=0, \n",
        "                                                       threshold=LR_TSH, threshold_mode=TSH_MODE, verbose=True)\n",
        "\n",
        "prev_valid_loss = 100\n",
        "trigger_times = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_loss = 0.0\n",
        "\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "\n",
        "  for data, labels in train_loader:\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(data)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    train_total += labels.size(0)\n",
        "    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # print statistics\n",
        "    train_loss += loss.item()\n",
        "    \n",
        "  valid_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, labels in valid_loader:\n",
        "      if torch.cuda.is_available():\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = loss_fn(outputs,labels)\n",
        "        valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        val_total += labels.size(0)\n",
        "        val_correct += (predicted == labels).sum().item()\n",
        "  \n",
        "\n",
        "  # Early stopping\n",
        "  if valid_loss > prev_valid_loss:\n",
        "    trigger_times += 1\n",
        "    print('trigger times:', trigger_times)\n",
        "\n",
        "    if trigger_times >= ES_PATIENCE:\n",
        "      print('Early stopping!')\n",
        "      break\n",
        "\n",
        "  else:\n",
        "    trigger_times = 0\n",
        "\n",
        "  prev_valid_loss = valid_loss\n",
        "\n",
        "  scheduler.step(valid_loss)\n",
        "\n",
        "  val_accuracy = (100 * val_correct) // val_total\n",
        "\n",
        "  print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Training Accuracy: {100 * train_correct // train_total}' )\n",
        "  print(f'\\t\\t Validation Loss: {valid_loss / len(valid_loader)} \\t\\t Validation Accuracy: {val_accuracy}')\n",
        "  if val_accuracy > top_accuracy:\n",
        "      print(f'Validation Accuracy Increased({top_accuracy:.6f}--->{val_accuracy:.6f}) \\t Saving The Model')\n",
        "      top_accuracy = val_accuracy\n",
        "      # Save Model\n",
        "      torch.save(model, MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "TtjR-IZkaq2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdaa1305-a354-4652-a09e-77a169b1bd19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t\t Training Loss: 0.4750605475496162 \t\t Training Accuracy: 82\n",
            "\t\t Validation Loss: 0.02435187713519947 \t\t Validation Accuracy: 66\n",
            "trigger times: 1\n",
            "Epoch 00002: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 2 \t\t Training Loss: 0.4264716088941151 \t\t Training Accuracy: 85\n",
            "\t\t Validation Loss: 0.03534060233348125 \t\t Validation Accuracy: 69\n",
            "Validation Accuracy Increased(67.000000--->69.000000) \t Saving The Model\n",
            "trigger times: 2\n",
            "Epoch 00003: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 3 \t\t Training Loss: 0.2756632675332102 \t\t Training Accuracy: 91\n",
            "\t\t Validation Loss: 0.19407832300340808 \t\t Validation Accuracy: 73\n",
            "Validation Accuracy Increased(69.000000--->73.000000) \t Saving The Model\n",
            "Epoch 00004: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch 4 \t\t Training Loss: 0.255732547017661 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.036120443730740935 \t\t Validation Accuracy: 70\n",
            "Epoch 5 \t\t Training Loss: 0.25635888620533726 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.007304676682562442 \t\t Validation Accuracy: 72\n",
            "trigger times: 1\n",
            "Epoch 00006: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch 6 \t\t Training Loss: 0.26908608376979826 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.01034641225595732 \t\t Validation Accuracy: 73\n",
            "trigger times: 2\n",
            "Epoch 7 \t\t Training Loss: 0.2521036311306737 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.0472039161501704 \t\t Validation Accuracy: 71\n",
            "Epoch 8 \t\t Training Loss: 0.25673342145640743 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.02284906925381841 \t\t Validation Accuracy: 73\n",
            "trigger times: 1\n",
            "Epoch 9 \t\t Training Loss: 0.2819678912955252 \t\t Training Accuracy: 91\n",
            "\t\t Validation Loss: 0.07524370986062127 \t\t Validation Accuracy: 73\n",
            "Epoch 10 \t\t Training Loss: 0.23993468535217372 \t\t Training Accuracy: 93\n",
            "\t\t Validation Loss: 0.04714360269340309 \t\t Validation Accuracy: 71\n",
            "Epoch 11 \t\t Training Loss: 0.24254491325806488 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.026504253213470046 \t\t Validation Accuracy: 69\n",
            "trigger times: 1\n",
            "Epoch 12 \t\t Training Loss: 0.24882593551142648 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.040974405971733296 \t\t Validation Accuracy: 72\n",
            "Epoch 13 \t\t Training Loss: 0.26550972239876336 \t\t Training Accuracy: 91\n",
            "\t\t Validation Loss: 0.03524252772331238 \t\t Validation Accuracy: 72\n",
            "trigger times: 1\n",
            "Epoch 14 \t\t Training Loss: 0.2633613910526037 \t\t Training Accuracy: 91\n",
            "\t\t Validation Loss: 0.1983961479083912 \t\t Validation Accuracy: 72\n",
            "Epoch 15 \t\t Training Loss: 0.2729429393922063 \t\t Training Accuracy: 91\n",
            "\t\t Validation Loss: 0.0488707278225873 \t\t Validation Accuracy: 73\n",
            "Epoch 16 \t\t Training Loss: 0.26156831183894114 \t\t Training Accuracy: 90\n",
            "\t\t Validation Loss: 0.03154092946568051 \t\t Validation Accuracy: 74\n",
            "Validation Accuracy Increased(73.000000--->74.000000) \t Saving The Model\n",
            "trigger times: 1\n",
            "Epoch 17 \t\t Training Loss: 0.23216533251106738 \t\t Training Accuracy: 93\n",
            "\t\t Validation Loss: 0.0741396208067198 \t\t Validation Accuracy: 69\n",
            "Epoch 18 \t\t Training Loss: 0.24799685617062178 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.01144051068537944 \t\t Validation Accuracy: 72\n",
            "trigger times: 1\n",
            "Epoch 19 \t\t Training Loss: 0.24294020137326283 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.03382460571624137 \t\t Validation Accuracy: 74\n",
            "trigger times: 2\n",
            "Epoch 20 \t\t Training Loss: 0.23190185268494215 \t\t Training Accuracy: 93\n",
            "\t\t Validation Loss: 0.053563467554143956 \t\t Validation Accuracy: 74\n",
            "Epoch 21 \t\t Training Loss: 0.24192512499337845 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.03934163660616488 \t\t Validation Accuracy: 74\n",
            "Epoch 22 \t\t Training Loss: 0.25731972054663027 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.02722341869328473 \t\t Validation Accuracy: 73\n",
            "Epoch 23 \t\t Training Loss: 0.2513859606432644 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.004876344489890176 \t\t Validation Accuracy: 71\n",
            "trigger times: 1\n",
            "Epoch 24 \t\t Training Loss: 0.254299407274547 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.18224194243147568 \t\t Validation Accuracy: 72\n",
            "Epoch 25 \t\t Training Loss: 0.2541823216290636 \t\t Training Accuracy: 91\n",
            "\t\t Validation Loss: 0.028990433828250783 \t\t Validation Accuracy: 71\n",
            "trigger times: 1\n",
            "Epoch 26 \t\t Training Loss: 0.25125603240660643 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.06714753685770808 \t\t Validation Accuracy: 73\n",
            "Epoch 27 \t\t Training Loss: 0.27371935964646665 \t\t Training Accuracy: 91\n",
            "\t\t Validation Loss: 0.01770183162109272 \t\t Validation Accuracy: 75\n",
            "Validation Accuracy Increased(74.000000--->75.000000) \t Saving The Model\n",
            "trigger times: 1\n",
            "Epoch 28 \t\t Training Loss: 0.24785154778510332 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.06752420116115261 \t\t Validation Accuracy: 71\n",
            "Epoch 29 \t\t Training Loss: 0.2537546506151557 \t\t Training Accuracy: 92\n",
            "\t\t Validation Loss: 0.02207462368784724 \t\t Validation Accuracy: 72\n",
            "trigger times: 1\n",
            "Epoch 30 \t\t Training Loss: 0.2473658528517593 \t\t Training Accuracy: 93\n",
            "\t\t Validation Loss: 0.026750489666655258 \t\t Validation Accuracy: 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test on whole test-set"
      ],
      "metadata": {
        "id": "hdh8Dx7-CJSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "model = torch.load(MODEL_PATH).to(device)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        # the class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "print(f'Accuracy on the 733 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "ayWihINZBcz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e4be178-c049-476f-aef6-cc364679652a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the 733 test images: 73 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in CLASSES}\n",
        "total_pred = {classname: 0 for classname in CLASSES}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[CLASSES[label]] += 1\n",
        "            total_pred[CLASSES[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "bu3rBoIaB7Ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e527177-2c83-41fe-9054-7adca3cc07c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: No DR is 87.2 %\n",
            "Accuracy for class: Mild  is 64.4 %\n",
            "Accuracy for class: Moderate is 55.8 %\n",
            "Accuracy for class: Severe is 47.4 %\n",
            "Accuracy for class: Proliferative DR is 50.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PN_D0V55CsRb"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}