{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "nwRzwl-ziuG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisation"
      ],
      "metadata": {
        "id": "-ND7DLmjiwHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RxNcDSRXknR8"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt \n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Global Parameters\n",
        "CLASSES = [ \"No DR\", \"Mild\", \"Moderate\", \"Severe\", \"Proliferative DR\" ]\n",
        "N_CLASSES = 5\n",
        "IMG_SIZE = (512, 512)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup with Dataset Download (11G)"
      ],
      "metadata": {
        "id": "OpLfBjQ6jRD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWHtA3idktPX"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/APTOS2019'\n",
        "TRAIN_PATH = DATASET_PATH + \"/train_images/\"\n",
        "TRAIN_PREP_PATH = DATASET_PATH + \"/train_preprocessed/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1nAz6IUq9OJ309bgxXLz38zbDCKXnnb4r\n",
        "!unzip APTOS2019.zip"
      ],
      "metadata": {
        "id": "trO4iRTKjXtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup with Google Drive Access"
      ],
      "metadata": {
        "id": "FDp3A8AhjZZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFw5FXuIk32k",
        "outputId": "7f4176ed-8e63-4671-e7b5-ec8fb1caa153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "CWD: /content/drive/My Drive/University Of Stirling/Dissertation/retinal-rec/Datasets/APTOS2019\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Parameters\n",
        "DATASET_PATH = '/content/drive/My Drive/University Of Stirling/Dissertation/retinal-rec/Datasets/APTOS2019'\n",
        "TRAIN_PATH = DATASET_PATH + \"/train_images/\"\n",
        "TRAIN_PREP_PATH = DATASET_PATH + \"/train_preprocessed/\"\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(DATASET_PATH)\n",
        "print(\"CWD:\",os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Implementation"
      ],
      "metadata": {
        "id": "uuk6v8TeuQtv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGwGGMcyxiti"
      },
      "source": [
        "In preparation for the NN, it is necessary to create the correct directory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUzIzdkex1gm"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DATASET_PATH + \"/train\"):\n",
        "  os.mkdir(DATASET_PATH + \"/train\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/0\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/1\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/2\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/3\")\n",
        "  os.mkdir(DATASET_PATH + \"/train/4\")\n",
        "\n",
        "if not os.path.exists(DATASET_PATH + \"/validation\"):\n",
        "  os.mkdir(DATASET_PATH + \"/validation\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/0\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/1\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/2\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/3\")\n",
        "  os.mkdir(DATASET_PATH + \"/validation/4\")\n",
        "\n",
        "if not os.path.exists(DATASET_PATH + \"/test\"):\n",
        "  os.mkdir(DATASET_PATH + \"/test\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/0\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/1\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/2\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/3\")\n",
        "  os.mkdir(DATASET_PATH + \"/test/4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KAjfl4Tk9UM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f49448e-8f69-419c-ac5a-fb0282cecb88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2050, 2) (879, 2) (733, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "def read_dataset():\n",
        "  df = pd.read_csv(DATASET_PATH + \"/train.csv\")\n",
        "  \n",
        "  # Train-test split\n",
        "  train, test = train_test_split(df, test_size=.2)\n",
        "  train, valid = train_test_split(train, test_size=.3)\n",
        "\n",
        "  for image in train.values:\n",
        "    shutil.copyfile(TRAIN_PREP_PATH + image[0] + \".png\", DATASET_PATH + \"/train/\" + str(image[1]) + \"/\" + image[0] + \".png\")\n",
        "\n",
        "  for image in valid.values:\n",
        "    shutil.copyfile(TRAIN_PREP_PATH + image[0] + \".png\", DATASET_PATH + \"/validation/\" + str(image[1]) + \"/\" + image[0] + \".png\")\n",
        "\n",
        "  for image in test.values:\n",
        "    shutil.copyfile(TRAIN_PREP_PATH + image[0] + \".png\", DATASET_PATH + \"/test/\" + str(image[1]) + \"/\" + image[0] + \".png\")\n",
        "  \n",
        "  return (train, valid, test)\n",
        "\n",
        "\n",
        "train, valid, test = read_dataset()\n",
        "\n",
        "print(train.shape, valid.shape, test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdri7jX-27Pu",
        "outputId": "ecbda774-72cf-45cb-fae4-e47feb1a9d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2050 images belonging to 5 classes.\n",
            "Found 879 images belonging to 5 classes.\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_66 (Conv2D)          (None, 62, 62, 128)       46592     \n",
            "                                                                 \n",
            " batch_normalization_65 (Bat  (None, 62, 62, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_40 (MaxPoolin  (None, 31, 31, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_67 (Conv2D)          (None, 31, 31, 256)       819456    \n",
            "                                                                 \n",
            " batch_normalization_66 (Bat  (None, 31, 31, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_41 (MaxPoolin  (None, 10, 10, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_68 (Conv2D)          (None, 10, 10, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_67 (Bat  (None, 10, 10, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_69 (Conv2D)          (None, 10, 10, 256)       65792     \n",
            "                                                                 \n",
            " batch_normalization_68 (Bat  (None, 10, 10, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_70 (Conv2D)          (None, 10, 10, 256)       65792     \n",
            "                                                                 \n",
            " batch_normalization_69 (Bat  (None, 10, 10, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_42 (MaxPoolin  (None, 5, 5, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 1024)              6554624   \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 5)                 5125      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,201,669\n",
            "Trainable params: 9,199,365\n",
            "Non-trainable params: 2,304\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "65/65 [==============================] - 356s 5s/step - loss: 1.8287 - accuracy: 0.4751 - val_loss: 4.5838 - val_accuracy: 0.4653\n",
            "Epoch 2/10\n",
            "65/65 [==============================] - 355s 5s/step - loss: 1.2918 - accuracy: 0.5741 - val_loss: 4.2181 - val_accuracy: 0.4653\n",
            "Epoch 3/10\n",
            "65/65 [==============================] - 354s 5s/step - loss: 1.0348 - accuracy: 0.6259 - val_loss: 1.4608 - val_accuracy: 0.4676\n",
            "Epoch 4/10\n",
            "65/65 [==============================] - 351s 5s/step - loss: 0.9537 - accuracy: 0.6483 - val_loss: 1.4897 - val_accuracy: 0.4744\n",
            "Epoch 5/10\n",
            "65/65 [==============================] - 350s 5s/step - loss: 0.9225 - accuracy: 0.6800 - val_loss: 1.1779 - val_accuracy: 0.5586\n",
            "Epoch 6/10\n",
            "65/65 [==============================] - 352s 5s/step - loss: 0.8754 - accuracy: 0.6761 - val_loss: 1.2927 - val_accuracy: 0.5711\n",
            "Epoch 7/10\n",
            "65/65 [==============================] - 353s 5s/step - loss: 0.8343 - accuracy: 0.6941 - val_loss: 1.5278 - val_accuracy: 0.4243\n",
            "Epoch 8/10\n",
            "65/65 [==============================] - 351s 5s/step - loss: 0.8998 - accuracy: 0.6712 - val_loss: 0.9957 - val_accuracy: 0.6667\n",
            "Epoch 9/10\n",
            "65/65 [==============================] - 351s 5s/step - loss: 0.8144 - accuracy: 0.7034 - val_loss: 1.8776 - val_accuracy: 0.3754\n",
            "Epoch 10/10\n",
            "65/65 [==============================] - 357s 5s/step - loss: 0.8340 - accuracy: 0.6990 - val_loss: 2.0593 - val_accuracy: 0.3743\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8e4375f290>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH + \"/train.csv\")\n",
        "  \n",
        "# Train-test split\n",
        "#train, test = train_test_split(df, test_size=.2)\n",
        "\n",
        "# create a data generator\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "# load and iterate training dataset\n",
        "train_it = datagen.flow_from_directory(DATASET_PATH + \"/train/\", class_mode='categorical', batch_size=32)\n",
        "valid_it = datagen.flow_from_directory(DATASET_PATH + \"/validation/\", class_mode='categorical', batch_size=32)\n",
        "\n",
        "model=keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(256,256,3)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3)),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(2,2)), \n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1024,activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1024,activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(5,activation='softmax')  \n",
        "])\n",
        "\n",
        "#model.build((256,512,512,3))\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001), metrics=[\"accuracy\"], run_eagerly=True)\n",
        "\n",
        "\n",
        "model.fit(train_it, validation_data=valid_it, epochs=10, verbose=1) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_it = datagen.flow_from_directory(DATASET_PATH + \"/test/\", class_mode='categorical', batch_size=32, shuffle = False)\n",
        "\n",
        "predictions = model.predict(test_it)\n",
        "loss, acc = model.evaluate(test_it, verbose=1)\n",
        "print(loss,acc)"
      ],
      "metadata": {
        "id": "yOoyB9Q5Kvet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66607762-d8b9-496c-fe8e-e6c0c7a65079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 733 images belonging to 5 classes.\n",
            "1260\n",
            "23/23 [==============================] - 34s 1s/step - loss: 2.0655 - accuracy: 0.3602\n",
            "2.065523386001587 0.36016371846199036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Implementation"
      ],
      "metadata": {
        "id": "Gyzq0W9rfCX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader \n",
        "import random\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "TnLwTGuxM4fE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "JFf_8UKcfBVh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path + \".png\")\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Lambda(lambda image: image.convert('RGB')),\n",
        "    # Data Augmentation\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5413734, 0.5413734, 0.5413734], [0.17313044, 0.17313044, 0.17313044])\n",
        "    ])\n",
        "\n",
        "dataset = CustomImageDataset(DATASET_PATH + \"/train.csv\", TRAIN_PREP_PATH, transform = train_transforms)\n"
      ],
      "metadata": {
        "id": "C222Z5Gzn5w2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "#Train-test-validation split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = (len(dataset) - train_size)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_size = int(0.8 * train_size)\n",
        "valid_size = (len(dataset) - train_size - test_size)\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Use a Weighted Sampler to fix class imbalances\n",
        "\n",
        "class_count = np.zeros(5)\n",
        "for image, label in train_dataset:\n",
        "  class_count[label] +=1\n",
        "\n",
        "class_weights = 1. / class_count\n",
        "weights = np.array([])\n",
        "\n",
        "for image, label in train_dataset:\n",
        "  weights = np.append(weights, class_weights[label])\n",
        "  \n",
        "weights = torch.from_numpy(weights)\n",
        "\n",
        "weighted_sampler = WeightedRandomSampler(weights=weights,num_samples=len(train_dataset),replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, sampler = weighted_sampler)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "3Sx_0ZJT5S1i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.alexnet(pretrained=True)\n",
        "model.fc = nn.Linear(4096, N_CLASSES)\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "xHXu_ULNhHsD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "b2f5055fe55843938b0af8b3d090b529",
            "ae933abb5b4b45fd81c79aa35e22d203",
            "f72826bcf4bc4e0a8aacf02a7f33ac0a",
            "28a12604f2f2419abd661e511d91fd83",
            "626eda76501b445aa7f12539a45edbd6",
            "1a5c2d8632c945b3b1e24106f23670ae",
            "c3f842e3366b473ab0df58a2e8e54288",
            "b318681a0f494da591141c52cf024737",
            "5875b46b67844bc2b59ecd6b954362e4",
            "707b9a4741774d51bbee1b958af2987e",
            "cdc6386c517e4d45a48597d74cd0daf2"
          ]
        },
        "outputId": "09666d27-d9a5-46da-9c73-c20c8546f846"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2f5055fe55843938b0af8b3d090b529"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=4096, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "num_epochs = 10\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Ensures network is in train mode\n",
        "model.train()\n",
        "\n",
        "best_model = model\n",
        "best_accuracy = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs): \n",
        "    train_loss_ep = 0.0\n",
        "    val_loss_ep = 0.0\n",
        "    \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    \n",
        "    for data, targets in iter(train_loader):\n",
        "\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Acquires the network's best guesses at each class\n",
        "        results = model(data)\n",
        "\n",
        "        ## Forward Pass\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(results,targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_ep += loss.item()\n",
        "\n",
        "        _, predictions = results.max(1)\n",
        "        num_correct += (predictions == targets).sum()\n",
        "        num_samples += predictions.size(0)\n",
        "\n",
        "    train_accuracy = float(num_correct) / float(num_samples) * 100\n",
        "\n",
        "    print(f\"Epoch [{epoch}] --- train loss: {train_loss_ep/len(train_loader)}, train accuracy: {train_accuracy}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for data, targets in iter(valid_loader):\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "\n",
        "            # Acquires the network's best guesses at each class\n",
        "            results = model(data)\n",
        "\n",
        "            _, predictions = results.max(1)\n",
        "            num_correct += (predictions == targets).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "        \n",
        "        val_accuracy = float(num_correct) / float(num_samples) * 100\n",
        "        print(f\"Epoch [{epoch}] --- val accuracy: {val_accuracy}\")\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "          # Save model\n",
        "          best_accuracy = val_accuracy\n",
        "          best_model = copy.deepcopy(model)\n",
        "\n",
        "torch.save(best_model, 'trained-model.pt') \n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print (\"Spent \", elapsed_time, \" seconds training for \", num_epochs, \" epoch(s).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSFOvPavOtnf",
        "outputId": "6d790fae-75ab-425e-f248-9a258d8704d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0] --- train loss: 1.8506036925641343, train accuracy: 20.6145966709347\n",
            "Epoch [0] --- val accuracy: 39.59044368600683\n",
            "Epoch [1] --- train loss: 1.6317602619783056, train accuracy: 19.504908237302605\n",
            "Epoch [1] --- val accuracy: 11.262798634812286\n",
            "Epoch [2] --- train loss: 1.622283884282812, train accuracy: 21.21212121212121\n",
            "Epoch [2] --- val accuracy: 12.627986348122866\n",
            "Epoch [3] --- train loss: 1.6182733231030226, train accuracy: 21.169440887750746\n",
            "Epoch [3] --- val accuracy: 29.522184300341298\n",
            "Epoch [4] --- train loss: 1.6175735729139413, train accuracy: 19.718309859154928\n",
            "Epoch [4] --- val accuracy: 30.887372013651877\n",
            "Epoch [5] --- train loss: 1.6147411121850128, train accuracy: 21.340162185232607\n",
            "Epoch [5] --- val accuracy: 29.351535836177472\n",
            "Epoch [6] --- train loss: 1.6140084836263298, train accuracy: 19.80367050789586\n",
            "Epoch [6] --- val accuracy: 23.378839590443686\n",
            "Epoch [7] --- train loss: 1.6145725758816194, train accuracy: 20.52923602219377\n",
            "Epoch [7] --- val accuracy: 9.556313993174061\n",
            "Epoch [8] --- train loss: 1.611194049132155, train accuracy: 20.69995731967563\n",
            "Epoch [8] --- val accuracy: 11.262798634812286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = best_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Test accuracy: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkuUD732ZTYH",
        "outputId": "e4cb8b19-dd37-4ee5-86f6-db24e4858182"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 43 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l06Y6XgBc-eU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Training_AlexNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4RFMtD605SufSGOLRwhbc"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2f5055fe55843938b0af8b3d090b529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae933abb5b4b45fd81c79aa35e22d203",
              "IPY_MODEL_f72826bcf4bc4e0a8aacf02a7f33ac0a",
              "IPY_MODEL_28a12604f2f2419abd661e511d91fd83"
            ],
            "layout": "IPY_MODEL_626eda76501b445aa7f12539a45edbd6"
          }
        },
        "ae933abb5b4b45fd81c79aa35e22d203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a5c2d8632c945b3b1e24106f23670ae",
            "placeholder": "​",
            "style": "IPY_MODEL_c3f842e3366b473ab0df58a2e8e54288",
            "value": "100%"
          }
        },
        "f72826bcf4bc4e0a8aacf02a7f33ac0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b318681a0f494da591141c52cf024737",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5875b46b67844bc2b59ecd6b954362e4",
            "value": 244408911
          }
        },
        "28a12604f2f2419abd661e511d91fd83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_707b9a4741774d51bbee1b958af2987e",
            "placeholder": "​",
            "style": "IPY_MODEL_cdc6386c517e4d45a48597d74cd0daf2",
            "value": " 233M/233M [00:02&lt;00:00, 141MB/s]"
          }
        },
        "626eda76501b445aa7f12539a45edbd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a5c2d8632c945b3b1e24106f23670ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3f842e3366b473ab0df58a2e8e54288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b318681a0f494da591141c52cf024737": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5875b46b67844bc2b59ecd6b954362e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "707b9a4741774d51bbee1b958af2987e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc6386c517e4d45a48597d74cd0daf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}