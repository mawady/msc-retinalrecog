{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvNEXT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7En8mOAo3LJU"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPAPwD2nOxRcpKejOgnynj/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ConvNEXT for DR detection"
      ],
      "metadata": {
        "id": "BqTdYbiDufny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "v6T0uIkA2aCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive Access"
      ],
      "metadata": {
        "id": "FqjI4jX32eZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wasxZn3ue1t",
        "outputId": "a99c9c60-28c4-420f-bc30-602eb0090555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "CWD: /content/drive/My Drive/University Of Stirling/Dissertation/ConvNEXT/APTOS2019\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Parameters\n",
        "DATASET_PATH = '/content/drive/My Drive/University Of Stirling/Dissertation/ConvNEXT/APTOS2019'\n",
        "PREP_PATH = DATASET_PATH + \"/preprocessed/\"\n",
        "\n",
        "# Load Dataset From Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(DATASET_PATH)\n",
        "print(\"CWD:\",os.getcwd())\n",
        "\n",
        "if not os.path.exists(PREP_PATH):\n",
        "  os.mkdir(PREP_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup"
      ],
      "metadata": {
        "id": "7En8mOAo3LJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Bt1_BXOWV7sf",
        "outputId": "f084814e-dc6e-4c10-96ba-de656f4d5ed1"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm_dWwHWWU7U",
        "outputId": "5f1ce771-5277-41be-9f91-3d42348715e1"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%bash\n",
        "\n",
        "pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "git clone https://github.com/facebookresearch/ConvNeXt\n",
        "pip install timm==0.3.2 tensorboardX six\n",
        "pip install submitit\n",
        "'''"
      ],
      "metadata": {
        "id": "6H8RHXnH3N2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and global parameters"
      ],
      "metadata": {
        "id": "ifpMFFxX2lkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models import ConvNeXt_Tiny_Weights\n",
        "from torch import nn\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader \n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "M8h_iHOx2sX5"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 10\n",
        "NUM_EPOCHS = 10\n",
        "MODEL_PATH=\"/content/drive/My Drive/University Of Stirling/Dissertation/ConvNEXT/checkpoints/checkpoint.pth\"\n",
        "CLASSES = [ \"No DR\", \"Mild\", \"Moderate\", \"Severe\", \"Proliferative DR\" ]"
      ],
      "metadata": {
        "id": "c4XnQDs22u1c"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "fIl8zMTjz05Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# ref: https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy/notebook\n",
        "# ref: https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy/notebook\n",
        "# ref for circle crop: https://github.com/debayanmitra1993-data/Blindness-Detection-Diabetic-Retinopathy-/blob/master/research_paper_implementation.ipynb\n",
        "def crop_image_from_gray(img,tol=7):\n",
        "    if img.ndim ==2:\n",
        "        mask = img>tol\n",
        "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "    elif img.ndim==3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mask = gray_img>tol\n",
        "        \n",
        "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
        "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
        "            return img # return original image\n",
        "        else:\n",
        "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
        "            img = np.stack([img1,img2,img3],axis=-1)\n",
        "        return img\n",
        "\n",
        "def circle_crop(img, sigmaX = 30):   \n",
        "    \"\"\"\n",
        "    Create circular crop around image centre    \n",
        "    \"\"\"    \n",
        "    img = crop_image_from_gray(img)    \n",
        "    \n",
        "    height, width, depth = img.shape    \n",
        "    \n",
        "    x = int(width/2)\n",
        "    y = int(height/2)\n",
        "    r = np.amin((x,y))\n",
        "    \n",
        "    circle_img = np.zeros((height, width), np.uint8)\n",
        "    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n",
        "    img = cv2.bitwise_and(img, img, mask=circle_img)\n",
        "    img = crop_image_from_gray(img)\n",
        "    return img \n",
        "\n",
        "def preprocess(id_code):\n",
        "  path = DATASET_PATH + \"/train_images/\" + id_code + \".png\"\n",
        "\n",
        "  if(os.path.isfile(path) == False):\n",
        "    print(id_code + \" does not exist!\")\n",
        "    return\n",
        "\n",
        "  img = cv2.imread(path)\n",
        "\n",
        "\n",
        "  # Circle crop\n",
        "  img = circle_crop(img)\n",
        "\n",
        "  # Resize the image\n",
        "  img = cv2.resize(img, (224, 224))\n",
        "\n",
        "  # Extract Green Channel\n",
        "  img[:,:,0] = 0\n",
        "  img[:,:,2] = 0\n",
        "\n",
        "  # Convert to Greyscale\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Apply Gaussian Blur\n",
        "  img = cv2.addWeighted(img,4, cv2.GaussianBlur( img , (0,0) , 512/10) ,-4 ,128)\n",
        "  \n",
        "  # Perform histogram equalization\n",
        "\n",
        "  clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8,8))\n",
        "  img = clahe.apply(img)\n",
        "\n",
        "  cv2.imwrite(PREP_PATH + id_code + \".png\", img)\n",
        "\n",
        "\n",
        "for id_code in dataset[\"id_code\"]:\n",
        "  preprocess(id_code) \n",
        "  '''"
      ],
      "metadata": {
        "id": "IDgCkcf1z0NF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "class DrDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = os.path.join(self.root_dir, self.labels.iloc[idx, 0])\n",
        "        img = read_image(path + \".png\")\n",
        "        lbl = self.labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, lbl\n",
        "\n",
        "global_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Lambda(lambda image: image.convert('RGB')),\n",
        "    # Data Augmentation\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    #transforms.RandomRotation(20),\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.5413734, 0.5413734, 0.5413734], [0.17313044, 0.17313044, 0.17313044])\n",
        "    ])\n",
        "\n",
        "\n",
        "dataset = DrDetectionDataset(DATASET_PATH + \"/train.csv\", PREP_PATH, transform = global_transforms)"
      ],
      "metadata": {
        "id": "5-jyUfWeQsM1"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train-test-validation split\n",
        "\n",
        "# Set manual seed for reproducible results\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# 60% train, 20% test, 20% valid\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_val_size = (len(dataset) - train_size)\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_val_size])\n",
        "\n",
        "train_size = int(len(train_dataset) - test_val_size)\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_size, test_val_size])\n",
        "\n",
        "print(\"Train size: \", len(train_dataset))\n",
        "print(\"Val size: \", len(valid_dataset))\n",
        "print(\"Test size: \", len(test_dataset))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1yRiT6ycRw-",
        "outputId": "1b03e231-e9c4-46d7-d990-b4cb70df7079"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size:  2196\n",
            "Val size:  733\n",
            "Test size:  733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Use a Weighted Sampler to fix class imbalances\n",
        "\n",
        "class_count = np.zeros(5)\n",
        "for image, label in train_dataset:\n",
        "  class_count[label] +=1\n",
        "\n",
        "class_weights = 1. / class_count\n",
        "weights = np.array([])\n",
        "\n",
        "for image, label in train_dataset:\n",
        "  weights = np.append(weights, class_weights[label])\n",
        "  \n",
        "weights = torch.from_numpy(weights)\n",
        "weighted_sampler = WeightedRandomSampler(weights=weights,num_samples=len(train_dataset),replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, sampler = weighted_sampler)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
        "'''"
      ],
      "metadata": {
        "id": "Sf_I-FPGn0eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "6y1Ocn4xUOWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Model"
      ],
      "metadata": {
        "id": "uFKc_uo9ontb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.learnpytorch.io/06_pytorch_transfer_learning/\n",
        "from torchinfo import summary\n",
        "\n",
        "# Download pre-trained weights on IMAGENET\n",
        "weights = ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
        "\n",
        "model = torchvision.models.convnext_tiny(weights = weights).to(device)\n",
        "print(\"Base Model: \")\n",
        "print(summary(model=model, \n",
        "        input_size=(BATCH_SIZE, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV1jVxLtQl5o",
        "outputId": "cec1592b-191f-4141-a54d-f728ef2c21f5"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Model: \n",
            "=======================================================================================================================================\n",
            "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
            "=======================================================================================================================================\n",
            "ConvNeXt (ConvNeXt)                                     [10, 3, 224, 224]    [10, 1000]           --                   True\n",
            "├─Sequential (features)                                 [10, 3, 224, 224]    [10, 768, 7, 7]      --                   True\n",
            "│    └─Conv2dNormActivation (0)                         [10, 3, 224, 224]    [10, 96, 56, 56]     --                   True\n",
            "│    │    └─Conv2d (0)                                  [10, 3, 224, 224]    [10, 96, 56, 56]     4,704                True\n",
            "│    │    └─LayerNorm2d (1)                             [10, 96, 56, 56]     [10, 96, 56, 56]     192                  True\n",
            "│    └─Sequential (1)                                   [10, 96, 56, 56]     [10, 96, 56, 56]     --                   True\n",
            "│    │    └─CNBlock (0)                                 [10, 96, 56, 56]     [10, 96, 56, 56]     79,296               True\n",
            "│    │    └─CNBlock (1)                                 [10, 96, 56, 56]     [10, 96, 56, 56]     79,296               True\n",
            "│    │    └─CNBlock (2)                                 [10, 96, 56, 56]     [10, 96, 56, 56]     79,296               True\n",
            "│    └─Sequential (2)                                   [10, 96, 56, 56]     [10, 192, 28, 28]    --                   True\n",
            "│    │    └─LayerNorm2d (0)                             [10, 96, 56, 56]     [10, 96, 56, 56]     192                  True\n",
            "│    │    └─Conv2d (1)                                  [10, 96, 56, 56]     [10, 192, 28, 28]    73,920               True\n",
            "│    └─Sequential (3)                                   [10, 192, 28, 28]    [10, 192, 28, 28]    --                   True\n",
            "│    │    └─CNBlock (0)                                 [10, 192, 28, 28]    [10, 192, 28, 28]    306,048              True\n",
            "│    │    └─CNBlock (1)                                 [10, 192, 28, 28]    [10, 192, 28, 28]    306,048              True\n",
            "│    │    └─CNBlock (2)                                 [10, 192, 28, 28]    [10, 192, 28, 28]    306,048              True\n",
            "│    └─Sequential (4)                                   [10, 192, 28, 28]    [10, 384, 14, 14]    --                   True\n",
            "│    │    └─LayerNorm2d (0)                             [10, 192, 28, 28]    [10, 192, 28, 28]    384                  True\n",
            "│    │    └─Conv2d (1)                                  [10, 192, 28, 28]    [10, 384, 14, 14]    295,296              True\n",
            "│    └─Sequential (5)                                   [10, 384, 14, 14]    [10, 384, 14, 14]    --                   True\n",
            "│    │    └─CNBlock (0)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    │    └─CNBlock (1)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    │    └─CNBlock (2)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    │    └─CNBlock (3)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    │    └─CNBlock (4)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    │    └─CNBlock (5)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    │    └─CNBlock (6)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    │    └─CNBlock (7)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    │    └─CNBlock (8)                                 [10, 384, 14, 14]    [10, 384, 14, 14]    1,201,920            True\n",
            "│    └─Sequential (6)                                   [10, 384, 14, 14]    [10, 768, 7, 7]      --                   True\n",
            "│    │    └─LayerNorm2d (0)                             [10, 384, 14, 14]    [10, 384, 14, 14]    768                  True\n",
            "│    │    └─Conv2d (1)                                  [10, 384, 14, 14]    [10, 768, 7, 7]      1,180,416            True\n",
            "│    └─Sequential (7)                                   [10, 768, 7, 7]      [10, 768, 7, 7]      --                   True\n",
            "│    │    └─CNBlock (0)                                 [10, 768, 7, 7]      [10, 768, 7, 7]      4,763,136            True\n",
            "│    │    └─CNBlock (1)                                 [10, 768, 7, 7]      [10, 768, 7, 7]      4,763,136            True\n",
            "│    │    └─CNBlock (2)                                 [10, 768, 7, 7]      [10, 768, 7, 7]      4,763,136            True\n",
            "├─AdaptiveAvgPool2d (avgpool)                           [10, 768, 7, 7]      [10, 768, 1, 1]      --                   --\n",
            "├─Sequential (classifier)                               [10, 768, 1, 1]      [10, 1000]           --                   True\n",
            "│    └─LayerNorm2d (0)                                  [10, 768, 1, 1]      [10, 768, 1, 1]      1,536                True\n",
            "│    └─Flatten (1)                                      [10, 768, 1, 1]      [10, 768]            --                   --\n",
            "│    └─Linear (2)                                       [10, 768]            [10, 1000]           769,000              True\n",
            "=======================================================================================================================================\n",
            "Total params: 28,589,128\n",
            "Trainable params: 28,589,128\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 3.22\n",
            "=======================================================================================================================================\n",
            "Input size (MB): 6.02\n",
            "Forward/backward pass size (MB): 1312.75\n",
            "Params size (MB): 114.33\n",
            "Estimated Total Size (MB): 1433.10\n",
            "=======================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Tune the model parameters to fit our domain\n",
        "# Sets Feature extraction layers as not trainable\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "model.classifier[2] = torch.nn.Linear(in_features=768, out_features=5, bias=True)\n",
        "\n",
        "print(summary(model=model, \n",
        "        input_size=(32, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdFYOdA_ZMVv",
        "outputId": "8b4d5ac3-335a-422a-de79-f60298e36ff3"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================================================================================================\n",
            "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
            "=======================================================================================================================================\n",
            "ConvNeXt (ConvNeXt)                                     [32, 3, 224, 224]    [32, 5]              --                   Partial\n",
            "├─Sequential (features)                                 [32, 3, 224, 224]    [32, 768, 7, 7]      --                   False\n",
            "│    └─Conv2dNormActivation (0)                         [32, 3, 224, 224]    [32, 96, 56, 56]     --                   False\n",
            "│    │    └─Conv2d (0)                                  [32, 3, 224, 224]    [32, 96, 56, 56]     (4,704)              False\n",
            "│    │    └─LayerNorm2d (1)                             [32, 96, 56, 56]     [32, 96, 56, 56]     (192)                False\n",
            "│    └─Sequential (1)                                   [32, 96, 56, 56]     [32, 96, 56, 56]     --                   False\n",
            "│    │    └─CNBlock (0)                                 [32, 96, 56, 56]     [32, 96, 56, 56]     (79,296)             False\n",
            "│    │    └─CNBlock (1)                                 [32, 96, 56, 56]     [32, 96, 56, 56]     (79,296)             False\n",
            "│    │    └─CNBlock (2)                                 [32, 96, 56, 56]     [32, 96, 56, 56]     (79,296)             False\n",
            "│    └─Sequential (2)                                   [32, 96, 56, 56]     [32, 192, 28, 28]    --                   False\n",
            "│    │    └─LayerNorm2d (0)                             [32, 96, 56, 56]     [32, 96, 56, 56]     (192)                False\n",
            "│    │    └─Conv2d (1)                                  [32, 96, 56, 56]     [32, 192, 28, 28]    (73,920)             False\n",
            "│    └─Sequential (3)                                   [32, 192, 28, 28]    [32, 192, 28, 28]    --                   False\n",
            "│    │    └─CNBlock (0)                                 [32, 192, 28, 28]    [32, 192, 28, 28]    (306,048)            False\n",
            "│    │    └─CNBlock (1)                                 [32, 192, 28, 28]    [32, 192, 28, 28]    (306,048)            False\n",
            "│    │    └─CNBlock (2)                                 [32, 192, 28, 28]    [32, 192, 28, 28]    (306,048)            False\n",
            "│    └─Sequential (4)                                   [32, 192, 28, 28]    [32, 384, 14, 14]    --                   False\n",
            "│    │    └─LayerNorm2d (0)                             [32, 192, 28, 28]    [32, 192, 28, 28]    (384)                False\n",
            "│    │    └─Conv2d (1)                                  [32, 192, 28, 28]    [32, 384, 14, 14]    (295,296)            False\n",
            "│    └─Sequential (5)                                   [32, 384, 14, 14]    [32, 384, 14, 14]    --                   False\n",
            "│    │    └─CNBlock (0)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    │    └─CNBlock (1)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    │    └─CNBlock (2)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    │    └─CNBlock (3)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    │    └─CNBlock (4)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    │    └─CNBlock (5)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    │    └─CNBlock (6)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    │    └─CNBlock (7)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    │    └─CNBlock (8)                                 [32, 384, 14, 14]    [32, 384, 14, 14]    (1,201,920)          False\n",
            "│    └─Sequential (6)                                   [32, 384, 14, 14]    [32, 768, 7, 7]      --                   False\n",
            "│    │    └─LayerNorm2d (0)                             [32, 384, 14, 14]    [32, 384, 14, 14]    (768)                False\n",
            "│    │    └─Conv2d (1)                                  [32, 384, 14, 14]    [32, 768, 7, 7]      (1,180,416)          False\n",
            "│    └─Sequential (7)                                   [32, 768, 7, 7]      [32, 768, 7, 7]      --                   False\n",
            "│    │    └─CNBlock (0)                                 [32, 768, 7, 7]      [32, 768, 7, 7]      (4,763,136)          False\n",
            "│    │    └─CNBlock (1)                                 [32, 768, 7, 7]      [32, 768, 7, 7]      (4,763,136)          False\n",
            "│    │    └─CNBlock (2)                                 [32, 768, 7, 7]      [32, 768, 7, 7]      (4,763,136)          False\n",
            "├─AdaptiveAvgPool2d (avgpool)                           [32, 768, 7, 7]      [32, 768, 1, 1]      --                   --\n",
            "├─Sequential (classifier)                               [32, 768, 1, 1]      [32, 5]              --                   True\n",
            "│    └─LayerNorm2d (0)                                  [32, 768, 1, 1]      [32, 768, 1, 1]      1,536                True\n",
            "│    └─Flatten (1)                                      [32, 768, 1, 1]      [32, 768]            --                   --\n",
            "│    └─Linear (2)                                       [32, 768]            [32, 5]              3,845                True\n",
            "=======================================================================================================================================\n",
            "Total params: 27,823,973\n",
            "Trainable params: 5,381\n",
            "Non-trainable params: 27,818,592\n",
            "Total mult-adds (G): 10.29\n",
            "=======================================================================================================================================\n",
            "Input size (MB): 19.27\n",
            "Forward/backward pass size (MB): 4200.53\n",
            "Params size (MB): 111.27\n",
            "Estimated Total Size (MB): 4331.07\n",
            "=======================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "adpyl8PApNTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "#https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "min_valid_loss = np.inf\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_loss = 0.0\n",
        "  acc = 0.0\n",
        "  for data, labels in train_loader:\n",
        "    if torch.cuda.is_available():\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(data)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    train_loss += loss.item()\n",
        "    \n",
        "  valid_loss = 0.0\n",
        "  model.eval()     # Optional when not using Model Specific layer\n",
        "  for data, labels in valid_loader:\n",
        "    if torch.cuda.is_available():\n",
        "      data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "      outputs = model(data)\n",
        "      loss = loss_fn(outputs,labels)\n",
        "      valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "  print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(valid_loader)}')\n",
        "  if min_valid_loss > valid_loss:\n",
        "      print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "      min_valid_loss = valid_loss\n",
        "      # Saving State Dict\n",
        "      torch.save(model.state_dict(), MODEL_PATH)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtjR-IZkaq2i",
        "outputId": "cb373010-c056-49f8-819a-771b15236638"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t\t Training Loss: 0.5635965182191947 \t\t Validation Loss: 0.051229820058152485\n",
            "Validation Loss Decreased(inf--->3.791007) \t Saving The Model\n",
            "Epoch 2 \t\t Training Loss: 0.5356622040779753 \t\t Validation Loss: 0.08010792248957865\n",
            "Epoch 3 \t\t Training Loss: 0.5165525100617246 \t\t Validation Loss: 0.01064178387860994\n",
            "Validation Loss Decreased(3.791007--->0.787492) \t Saving The Model\n",
            "Epoch 4 \t\t Training Loss: 0.5002586168660359 \t\t Validation Loss: 0.010152339935302734\n",
            "Validation Loss Decreased(0.787492--->0.751273) \t Saving The Model\n",
            "Epoch 5 \t\t Training Loss: 0.48595560648563235 \t\t Validation Loss: 0.015128876711871173\n",
            "Epoch 6 \t\t Training Loss: 0.4731365736404603 \t\t Validation Loss: 0.013551122836164525\n",
            "Epoch 7 \t\t Training Loss: 0.46148304385556416 \t\t Validation Loss: 0.015974643665391045\n",
            "Epoch 8 \t\t Training Loss: 0.45076806027103555 \t\t Validation Loss: 0.012842123170156736\n",
            "Epoch 9 \t\t Training Loss: 0.4408275696110319 \t\t Validation Loss: 0.013875696707416224\n",
            "Epoch 10 \t\t Training Loss: 0.43153716473078185 \t\t Validation Loss: 0.011210523344374992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test on whole test-set"
      ],
      "metadata": {
        "id": "hdh8Dx7-CJSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        # the class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on the 733 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayWihINZBcz7",
        "outputId": "40c4352c-a4fe-4df9-a344-faa8bc05499c"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the 733 test images: 76 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in CLASSES}\n",
        "total_pred = {classname: 0 for classname in CLASSES}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[CLASSES[label]] += 1\n",
        "            total_pred[CLASSES[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu3rBoIaB7Ah",
        "outputId": "91bd48f4-a2e0-4dd0-d703-2b92bc3e62a3"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: No DR is 99.2 %\n",
            "Accuracy for class: Mild  is 38.9 %\n",
            "Accuracy for class: Moderate is 76.6 %\n",
            "Accuracy for class: Severe is 28.9 %\n",
            "Accuracy for class: Proliferative DR is 24.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PN_D0V55CsRb"
      },
      "execution_count": 160,
      "outputs": []
    }
  ]
}